Run Command:

python main.py --datasource=sinusoid --logdir=logs/sine1/ --metatrain_iterations=5000 --norm=None --update_batch_size=10 --limit_task True --regularize_penal 0.0

python main.py --datasource=sinusoid --logdir=logs/sine15/ --metatrain_iterations=5000 --norm=None --update_batch_size=10 --limit_task True --regularize_penal 0.0 --train=False --test_set=True

New run:
  python main.py --datasource=sinusoid --logdir=logs/sine4/ --metatrain_iterations=20000 --norm=None --update_batch_size=10 --limit_task True --regularize_penal 0.0 --meta_lr .00001 --update_lr .00001



Trying to get the multi-state to train:
  python main.py --datasource=sinusoid --logdir=logs/sine5/  --norm=None --update_batch_size=100 --limit_task True --regularize_penal 0.0 --meta_lr .0001 --update_lr .0001 --resume False --pretrain_iterations=70000 --metatrain_iterations=0


This did it:
  python main.py --datasource=sinusoid --logdir=logs/sine5/  --norm=None --update_batch_size=100 --limit_task True --regularize_penal 0.0 --meta_lr .0001 --update_lr .0001 --resume False --pretrain_iterations=70000 --metatrain_iterations=0

  Got loss to: 
    Validation results: 0.0010618774686008692, 0.023718448355793953
  Pretrain Iteration 21600: 0.0010948678, 0.023260498
  Pretrain Iteration 21700: 0.0011010369, 0.023254389
  (takes about 20k iterations)

Trying maml:
  python main.py --datasource=sinusoid --logdir=logs/sine5/  --norm=None --update_batch_size=100 --limit_task True --regularize_penal 0.0 --meta_lr .0001 --update_lr .0001 --resume False --metatrain_iterations=70000

python main.py --datasource=sinusoid --logdir=logs/sine5/ --metatrain_iterations=20000 --norm=None --update_batch_size=100 --limit_task True --regularize_penal 0.0 --meta_lr .00001 --update_lr .00001 --resume=True


python main.py --datasource=sinusoid --logdir=logs/sine5/  --norm=None --update_batch_size=100 --limit_task True --regularize_penal 0.0 --meta_lr .0001 --update_lr .0001 --resume True --metatrain_iterations=70000



CUDA_VISIBLE_DEVICES="0" python main.py --datasource=sinusoid --logdir=logs/sine5/  --norm=None --update_batch_size=100 --limit_task True --regularize_penal 0.0 --meta_lr .0001 --update_lr .0001 --resume False --metatrain_iterations=0 --pretrain_iterations=70000



python main.py --datasource=sinusoid --logdir=logs/sine5/  --norm=None --update_batch_size=100 --limit_task True --regularize_penal 0.0 --meta_lr .0001 --update_lr .0001 --resume True --metatrain_iterations=70000 --train=False --test_set=True


Next things to try on tor:

python main.py --datasource=sinusoid --logdir=logs/sine6/  --norm=None --update_batch_size=100 --regularize_penal 0.0 --meta_lr .001 --update_lr .001 --resume True --metatrain_iterations=70000 --train=False --test_set=True 


python main.py --datasource=sinusoid --logdir=logs/sine6/  --norm=None --update_batch_size=100 --regularize_penal 0.0 --meta_lr .001 --update_lr .001 --resume True --metatrain_iterations=70000 --limit_task=True

Try adjusting learning rate.
Try more gradient steps.
Try decreasing batch size
Try num_updates
  num_updates = 10

New run:
  python main.py --datasource=sinusoid --logdir=logs/sine7/  --norm=None --update_batch_size=100 --regularize_penal 0.0 --meta_lr .0001 --update_lr .0001 --resume True --metatrain_iterations=70000 --limit_task=True --num_updates=10

  CUDA_VISIBLE_DEVICES="1" python main.py --datasource=sinusoid --logdir=logs/sine8/  --norm=None --update_batch_size=100 --regularize_penal 0.0 --meta_lr .0001 --update_lr .0001 --resume True --metatrain_iterations=70000 --limit_task=True --num_updates=20


Relu?

python main.py --datasource=sinusoid --logdir=logs/sine6/  --norm=None --update_batch_size=100 --regularize_penal 0.0 --meta_lr .001 --update_lr .001 --resume True --metatrain_iterations=70000 --limit_task=True 


Things to look into:
  Trying to drop the loss. 

  Validation results: 0.0009190408163703978, 0.0009184564114548265
Iteration 68100: 0.0008531325, 0.00084577344


  python main.py --datasource=sinusoid --logdir=logs/sine6/  --norm=None --update_batch_size=100 --regularize_penal 0.0 --meta_lr .001 --update_lr .001 --resume True --metatrain_iterations=70000 --limit_task=True 



Maybe it is in how the data is being processed? It seems to be getting stuck in a local min.


python main.py --datasource=sinusoid --logdir=logs/sine6/  --norm=None --update_batch_size=100 --regularize_penal 0.0 --meta_lr .001 --update_lr .001 --resume False --metatrain_iterations=70000 --limit_task=True 



python main.py --datasource=sinusoid --logdir=logs/sine7/  --norm=None --update_batch_size=100 --regularize_penal 0.0 --meta_lr .001 --update_lr .001 --metatrain_iterations=70000 --num_updates=20 --meta_batch_size=25



python main.py --datasource=sinusoid --logdir=logs/sine7/  --norm=None --update_batch_size=100 --regularize_penal 0.00001 --meta_lr .001 --update_lr .001 --metatrain_iterations=70000 --num_updates=20 --meta_batch_size=2 --limit_task=True --resume=False







